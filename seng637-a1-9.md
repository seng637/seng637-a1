>   **SENG 637 - Software Testing, Reliability, and Quality**

**Lab. Report \#1 â€“ Introduction to Testing and Defect Tracking**

| Group: 9      |
|-----------------|
| Maheen                |   
| Dipu              |   
| Jasdeep               |   
| Dhruvi                |   


**Table of Contents**

[1 Introduction](#introduction)

[2 High-level description of the exploratory testing plan](#_Toc439194678)

[3 Comparison of exploratory and manual functional testing](#_Toc439194679)

[4 Notes and discussion of the peer reviews of defect reports](#_Toc439194680)

[5 How the pair testing was managed and team work/effort was divided](#_Toc439194681)

[6 Difficulties encountered, challenges overcome, and lessons
learned](#_Toc439194682)

[7 Comments/feedback on the lab and lab document itself](#_Toc439194683)

# Introduction

This assignment provided hands-on experience with software testing methodologies by testing an ATM simulation system, written in Java. The lab consisted of three main testing phases: exploratory testing, manual scripted testing, and regression testing.

Prior to this lab, our understanding of testing was primarily theoretical. It was known to us that exploratory testing involves ad-hoc testing without predefined test cases, allowing testers to freely explore the system based on their understanding and intuition. Manual functional testing, on the other hand, involves executing predefined test cases systematically to verify that the system meets its requirements. This lab allowed us to apply these concepts practically and understand their differences in real-world scenarios.

The system under test was an ATM simulation system with two versions (1.0 and 1.1), and we used Jira as our defect tracking system to document and manage all discovered defects throughout the testing process.

Throughout this assignment, we referred to the assignment description document, as it contained not only the requirements of the system under test (SUT), but also a guideline for which features to test.  

# High Level Description of the Exploratory Testing Plan

Our exploratory testing approach was designed to maximize coverage by leveraging the pair testing. The plan consisted of the following key elements:

## Test Approach

We divided our team into two pairs, with each pair focusing on different functional areas to ensure comprehensive coverage within the 30-minute timeframe. In order to have an understanding of the system for exploratory system, we first reviewed the high-level requirements of the system described in the appendix B of the assignment description. However, since this is only a mock ATM system with limited capabilities, we referred to the "System Under Test" section and "Familiarization with the ATM System" subsection of the assignment document to plan the exploratory testing. Rather than testing all functions in depth (which would be done in the scripted testing phase), we adopted a balanced approach that explored most major functions while conducting deeper testing on critical paths.


## Pairs and Functionality Focus

For exploratory testing we created two pairs - Maheen & Dipu in pair 1, and Jasdeep & Dhruvi in pair 2. 

- **Pair 1 Focus**: System startup, initial balance, and shutdown procedures, session management (card insertion, PIN validation, multi-transaction handling), and withdrawal operations
- **Pair 2 Focus**: Deposit operations, transfer operations, balance inquiry, and error handling scenarios

## Test Strategy
Our strategy emphasized:
1. **Common Path Testing**: First testing the most typical user workflows (successful transactions with valid inputs)
2. **Boundary Testing**: Testing limits such as maximum withdrawal amounts, minimum deposits, and account balance boundaries
3. **Exception Path Testing**: Deliberately triggering error conditions such as invalid PINs, insufficient funds, unreadable cards, and transaction cancellations
4. **State Transition Testing**: Verifying proper system behavior when transitioning between different states (on/off, idle/active, transaction types)

## Test Case Generation
Test cases were generated based on:
- Requirements outlined in Appendix B of the assignment
- Use cases provided in Appendix C
- Our intuition and experience with ATM systems
- Potential error scenarios and edge cases

## Documentation Approach
Each pair maintained real-time documentation of:
- Test scenarios executed
- Observed system behavior
- Defects discovered with complete reproduction steps
- Areas of the system covered

This exploratory plan allowed us to uncover defects that might not be captured by scripted test cases while maintaining systematic coverage of the system's key functionalities.

# Test Plan Details

## Test Types

### Exploratory Testing (Manual Non-Scripted)
- **Purpose**: To explore the functionalities of the system and uncover defects through ad-hoc testing without predefined test cases
- **Duration**: Approximately 30 minutes per pair
- **Approach**: Testers explored the system freely based on their understanding of requirements
- **Documentation**: Defects discovered were logged immediately in the bug tracking system

### Manual Scripted Testing
- **Purpose**: Systematically verify system functionality against predefined test cases
- **Test Suite**: 17 test cases provided in Appendix C of the assignment
- **Coverage**: 
  - System Startup and Shutdown (Test Cases 1-4)
  - Session Management (Test Cases 5-11)
  - Withdrawal Operations (Test Cases 12-17)
- **Documentation**: Defects were prefixed with "MFT:" in the summary field to distinguish them from exploratory testing defects

### Regression Testing
- **Purpose**: Verify bug fixes and identify new defects introduced in version 1.1
- **Scope**: Retest all defects found in version 1.0 and execute the scripted test suite again on version 1.1
- **Documentation**: Updated defect status (Resolved/Fixed or In-Progress) and reported new defects specific to version 1.1

## Scope of Testing

### In-Scope Functionalities

**System Operations**
- System startup with initial cash amount entry
- System shutdown when not servicing customers
- Connection establishment with the bank

**Session Management**
- ATM card reading and validation
- PIN entry and validation (including invalid PIN handling)
- Multi-transaction sessions
- Session termination and card ejection

**Transaction Types**
- **Withdrawal**: Account selection, amount selection, cash dispensing, receipt printing
- **Deposit**: Account selection, amount entry, envelope insertion
- **Transfer**: Source and destination account selection, amount entry
- **Balance Inquiry**: Account selection, balance display

**Security Features**
- Invalid PIN handling (3 attempts maximum)
- Card retention after failed authentication
- Transaction logging (excluding PINs)

**Error Handling**
- Insufficient cash in ATM
- Insufficient balance in account
- Unreadable card rejection
- Transaction cancellation

### Out-of-Scope
- Bank-side validation logic (external system)
- Physical hardware components
- Network communication protocols
- Database integrity
- Performance and load testing
- Security penetration testing

### Test Environment
- **System Under Test**: ATM System JAR files (v1.0 and v1.1)
- **Platform**: Windows 10, JDK 17.0.15.6
- **Test Data**:
  - Card Number: 1
  - PIN: 42
  - Initial Balances: Checking $100, Savings $1,000, Money Market $5,000
- **Defect Tracking**: Atlassian Jira or Azure DevOps

## Test Logistics

### Team Structure and Role Assignment

#### Exploratory Testing Phase
**Pair 1**: 
- **Tester**: Operates the system and executes test scenarios
- **Recorder**: Documents defects, observations, and test coverage
- **Focus Areas**: System startup/shutdown, session management, withdrawal operations

**Pair 2**:
- **Tester**: Operates the system and executes test scenarios
- **Recorder**: Documents defects, observations, and test coverage
- **Focus Areas**: Deposit operations, transfer operations, balance inquiry, error scenarios

#### Manual Scripted Testing Phase
- **Combined Group Activity**: All team members worked together
- **Driver**: One student operated the ATM system and executed test cases
- **Navigator**: Other students tracked test execution, verified results, and reported defects
- **Test Case Distribution**: Executed all 17 test cases sequentially
  - Test Cases 1-4: System Startup and Shutdown
  - Test Cases 5-11: Session Management
  - Test Cases 12-17: Withdrawal Transactions

#### Regression Testing Phase
- **Defect Verification**: Divided among all group members
- Each member retested assigned defects from version 1.0 in version 1.1
- **New Defect Discovery**: Combined group activity re-executing the scripted test suite
- **Status Updates**: Each member updated status of their assigned defects

### Defect Reporting Guidelines
All defect reports included:
1. Function being tested
2. Initial state of the system
3. Detailed steps to reproduce
4. Expected outcome
5. Actual outcome
6. Priority/Severity (Low, Medium, High, Critical)
7. Version of SUT (1.0 or 1.1)
8. Phase identifier (exploratory or "MFT:" prefix for manual scripted testing)

# Comparison of exploratory and manual functional testing

*[To be written based on your actual testing experience. Consider comparing aspects such as:]*
- *Effectiveness: Which approach found more defects or different types of defects?*
- *Efficiency: Time investment vs. defects discovered*
- *Coverage: Which areas of the system were better tested by each approach?*
- *Advantages and disadvantages of each method*
- *When each approach is most appropriate*
- *Your team's observations and insights from the lab*

**Note**: You need to submit a report generated by your defect tracking system, containing all defects recorded in the system.

# Notes and discussion of the peer reviews of defect reports

*[To be written based on your peer review process. Include:]*
- *How the peer review process was conducted*
- *Key findings from reviewing each pair's defect reports*
- *Any inconsistencies or issues identified during review*
- *How defects were consolidated or deduplicated*
- *Improvements made to defect reports based on peer feedback*

# How the pair testing was managed and team work/effort was divided 

*[To be written based on your actual team collaboration. Include:]*
- *Specific responsibilities of each team member*
- *How pairs coordinated during exploratory testing*
- *Communication methods used*
- *How the team handled disagreements or ambiguous situations*
- *Division of work during regression testing*
- *Overall assessment of the pair testing methodology*

# Difficulties encountered, challenges overcome, and lessons learned

*[To be written based on your actual experience. Consider discussing:]*
- *Technical difficulties with the SUT or tools*
- *Challenges in understanding requirements or expected behavior*
- *Issues with the defect tracking system*
- *Time management challenges*
- *How your team overcame these difficulties*
- *Key lessons learned about testing methodologies*
- *Insights gained about defect tracking and reporting*

# Comments/feedback on the lab and lab document itself

*[To be written based on your experience. Provide constructive feedback on:]*
- *Clarity of the lab instructions*
- *Usefulness of the assignment for learning testing concepts*
- *Suggestions for improvement*
- *Aspects that were particularly valuable or challenging*
- *Overall assessment of the lab experience*
