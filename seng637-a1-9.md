> **SENG 637 - Software Testing, Reliability, and Quality**

**Lab. Report \#1 – Introduction to Testing and Defect Tracking**

| Group: 9 |
| -------- |
| Maheen   |
| Dipu     |
| Jasdeep  |
| Dhruvi   |

**Table of Contents**

[1 Introduction](#introduction)

[2 High-level description of the exploratory testing plan](#high-level-description-of-the-exploratory-testing-plan)

[3 Comparison of exploratory and manual functional testing](#_Toc439194679)

[4 Notes and discussion of the peer reviews of defect reports](#_Toc439194680)

[5 How the pair testing was managed and team work/effort was divided](#_Toc439194681)

[6 Difficulties encountered, challenges overcome, and lessons
learned](#_Toc439194682)

[7 Comments/feedback on the lab and lab document itself](#_Toc439194683)

# Introduction

This assignment provided hands-on experience with software testing methodologies by testing an ATM simulation system, written in Java. The lab consisted of three main testing phases: exploratory testing, manual scripted testing, and regression testing.

Prior to this lab, our understanding of testing was primarily theoretical. It was known to us that exploratory testing involves ad-hoc testing without predefined test cases, allowing testers to freely explore the system based on their understanding and intuition. Manual functional testing, on the other hand, involves executing predefined test cases systematically to verify that the system meets its requirements. This lab allowed us to apply these concepts practically and understand their differences in real-world scenarios.

The system under test was an ATM simulation system with two versions (1.0 and 1.1), and we used Jira as our defect tracking system to document and manage all discovered defects throughout the testing process.

Throughout this assignment, we referred to the assignment description document, as it contained not only the requirements of the system under test (SUT), but also a guideline for which features to test.

# High Level Description of the Exploratory Testing Plan

Our exploratory testing approach was designed to maximize coverage by leveraging the pair testing. The plan consisted of the following key elements:

## Test Approach

We divided our team into two pairs, with each pair focusing on different functional areas to ensure comprehensive coverage within the 30-minute timeframe. In order to have an understanding of the system for exploratory system, we first reviewed the high-level requirements of the system described in the appendix B of the assignment description. However, since this is only a mock ATM system with limited capabilities, we referred to the "System Under Test" section and "Familiarization with the ATM System" subsection of the assignment document to plan the exploratory testing. Rather than testing all functions in depth (which would be done in the scripted testing phase), we adopted a simpler approach that tests out the basic functionalities, with some corner-cases that we deemed to be necessary at the moment. Again, since this is more or less an ad-hoc testing, not all operation scenarios were considered in this case. 

## Pairs and Functionality Focus

For exploratory testing we created two pairs - Maheen & Dipu in pair 1, and Jasdeep & Dhruvi in pair 2.

- **Pair 1 Focus**: System startup and shutdown procedures, session management (card insertion and PIN validation), and deposit operations.
- **Pair 2 Focus**: Withdrawal operations, transfer operations, and balance inquiry.

## Test Strategy

Our strategy emphasized:

1. **Common Path Testing**: First testing the most typical user workflows (successful transactions with valid inputs)
2. **Boundary Testing**: Testing limits such as maximum withdrawal amounts, minimum deposits, and account balance boundaries
3. **Exception Path Testing**: Deliberately triggering error conditions such as invalid PINs, insufficient funds, and transaction cancellations
4. **State Transition Testing**: Verifying proper system behavior when transitioning between different states (on/off and transaction types)

## Test Case Generation

Test cases were generated based on:

- Requirements outlined in Appendix B of the assignment
- Use cases provided in Appendix C
- Our intuition and experience with ATM systems (ad-hoc)
- Potential error scenarios and edge cases

## Documentation Approach

Each pair maintained real-time documentation of:

- Test scenarios executed
- Observed system behavior
- Defects discovered with complete reproduction steps
- Areas of the system covered

This exploratory plan allowed us to uncover defects that might not be captured by scripted test cases while maintaining systematic coverage of the system's key functionalities.

# Test Plan Details

## Test Types

### Exploratory Testing (Manual Non-Scripted)

- **Purpose**: To explore the functionalities of the system and uncover defects through ad-hoc testing without predefined test cases
- **Duration**: Approximately 30 minutes per pair
- **Approach**: Testers explored the system freely based on their understanding of requirements
- **Documentation**: Defects discovered were logged immediately in the bug tracking system

### Manual Scripted Testing

- **Purpose**: Systematically verify system functionality against predefined test cases
- **Test Suite**: 17 test cases provided in Appendix C of the assignment
- **Coverage**:
  - System Startup and Shutdown (Test Cases 1-4)
  - Session Management (Test Cases 5-11)
  - Withdrawal Operations (Test Cases 12-17)
- **Documentation**: Defects were prefixed with "MFT:" in the summary field to distinguish them from exploratory testing defects

### Regression Testing

- **Purpose**: Verify bug fixes and identify new defects introduced in version 1.1
- **Scope**: Retest all defects found in version 1.0 and execute the scripted test suite again on version 1.1
- **Documentation**: Updated defect status (Resolved/Fixed or In-Progress) and reported new defects specific to version 1.1

## Scope of Testing

### In-Scope Functionalities

**System Operations**

- System startup with initial cash amount entry
- System shutdown when not servicing customers
- Connection establishment with the bank

**Session Management**

- ATM card reading and validation
- PIN entry and validation (including invalid PIN handling)
- Multi-transaction sessions
- Session termination and card ejection

**Transaction Types**

- **Withdrawal**: Account selection, amount selection, cash dispensing, receipt printing
- **Deposit**: Account selection, amount entry, envelope insertion
- **Transfer**: Source and destination account selection, amount entry
- **Balance Inquiry**: Account selection, balance display

**Security Features**

- Invalid PIN handling (3 attempts maximum)
- Card retention after failed authentication
- Transaction logging (excluding PINs)

**Error Handling**

- Insufficient cash in ATM
- Insufficient balance in account
- Unreadable card rejection
- Transaction cancellation

### Out-of-Scope

- Bank-side validation logic (external system)
- Physical hardware components
- Network communication protocols
- Database integrity
- Performance and load testing
- Security penetration testing

### Test Environment

- **System Under Test**: ATM System JAR files (v1.0 and v1.1)
- **Platform**: Windows 10, JDK 17.0.15.6
- **Test Data**:
  - Card Number: 1
  - PIN: 42
  - Initial Balances: Checking $100, Savings $1,000, Money Market $5,000
- **Defect Tracking**: Atlassian Jira or Azure DevOps

## Test Logistics

### Team Structure and Role Assignment

#### Exploratory Testing Phase

**Pair 1**:

- **Tester**: Operates the system and executes test scenarios
- **Recorder**: Documents defects, observations, and test coverage
- **Focus Areas**: System startup/shutdown, session management, withdrawal operations

**Pair 2**:

- **Tester**: Operates the system and executes test scenarios
- **Recorder**: Documents defects, observations, and test coverage
- **Focus Areas**: Deposit operations, transfer operations, and balance inquiry

#### Manual Scripted Testing Phase

In the manual testing phase, all team members worked in unison. The roles for test executor, recorder, and organizer were rotated periodically. At a given moment, two members were in execution, one in recording (writing the found defect in Jira), and one in coordination (making sure that execution is being performed properly, and bug reports are being written with adherence to the given assignment instructions).

We executed all 40 test cases sequentially, as described in the assignment description:

- Test Cases 1-4: System startup and shutdown
- Test Cases 5-11: Session management
- Test Cases 12-18: Withdrawal operations
- Test Cases 19-25: Deposit operations
- Test Cases 26-32: Transfer operations
- Test Cases 33-35: Balance inquiry operations
- Test Cases 36-40: Invalid pin handling

(Note that we found some inconsistencies in the numbering order, but the aforementioned list should give a general idea of the test cases)

#### Regression Testing Phase

- **Defect Verification**: Divided among all group members
- Each member retested assigned defects from version 1.0 in version 1.1
- **New Defect Discovery**: Combined group activity re-executing the scripted test suite
- **Status Updates**: Each member updated status of their assigned defects

### Defect Reporting Guidelines

All defect reports included:

1. Function being tested
2. Initial state of the system
3. Detailed steps to reproduce
4. Expected outcome
5. Actual outcome
6. Priority/Severity (Low, Medium, High, Critical)
7. Version of SUT (1.0 or 1.1)
8. Phase identifier (exploratory or "MFT:" prefix for manual scripted testing)

# Comparison of exploratory and manual functional testing

_[To be written based on your actual testing experience. Consider comparing aspects such as:]_

- _Effectiveness: Which approach found more defects or different types of defects?_
- _Efficiency: Time investment vs. defects discovered_
- _Coverage: Which areas of the system were better tested by each approach?_
- _Advantages and disadvantages of each method_
- _When each approach is most appropriate_
- _Your team's observations and insights from the lab_

**Note**: You need to submit a report generated by your defect tracking system, containing all defects recorded in the system.

# Notes and discussion of the peer reviews of defect reports

_[To be written based on your peer review process. Include:]_

- _How the peer review process was conducted_
- _Key findings from reviewing each pair's defect reports_
- _Any inconsistencies or issues identified during review_
- _How defects were consolidated or deduplicated_
- _Improvements made to defect reports based on peer feedback_

# How the pair testing was managed and team work/effort was divided

_[To be written based on your actual team collaboration. Include:]_

- _Specific responsibilities of each team member_
- _How pairs coordinated during exploratory testing_
- _Communication methods used_
- _How the team handled disagreements or ambiguous situations_
- _Division of work during regression testing_
- _Overall assessment of the pair testing methodology_

# Difficulties encountered, challenges overcome, and lessons learned

_[To be written based on your actual experience. Consider discussing:]_

- _Technical difficulties with the SUT or tools_
- _Challenges in understanding requirements or expected behavior_
- _Issues with the defect tracking system_
- _Time management challenges_
- _How your team overcame these difficulties_
- _Key lessons learned about testing methodologies_
- _Insights gained about defect tracking and reporting_

# Comments/feedback on the lab and lab document itself

_[To be written based on your experience. Provide constructive feedback on:]_

- _Clarity of the lab instructions_
- _Usefulness of the assignment for learning testing concepts_
- _Suggestions for improvement_
- _Aspects that were particularly valuable or challenging_
- _Overall assessment of the lab experience_

  # 1 Introduction

This report presents a systematic evaluation of a Java-based Automated Teller Machine (ATM) simulation system using established software testing and defect tracking practices. The system under test (SUT) comprises two consecutive software releases (versions 1.0 and 1.1), allowing assessment of functional correctness, fault handling, and post-fix stability.

Testing activities targeted the ATM’s core operational and transactional behaviors, including system startup and shutdown, session control, cash withdrawals, deposits, account transfers, balance inquiries, and error handling under exceptional conditions. A combination of complementary testing approaches was employed. Exploratory testing was conducted to examine system behavior under unscripted interactions and to expose defects related to edge cases, invalid inputs, and state transitions. Manual scripted testing was subsequently applied using a predefined test suite to verify conformance with documented functional requirements. Regression testing on version 1.1 evaluated the resolution of previously reported defects and identified any regressions introduced by system modifications.

All identified defects were recorded, tracked, and updated using Jira, following standard defect reporting and lifecycle management practices. The assignment specification and published system requirements served as the authoritative basis for expected behavior, test coverage, and defect validation.

---

# 2 High-Level Description of the Exploratory Testing Plan

The exploratory testing phase was designed as an initial, discovery-driven evaluation of the ATM simulation system, with the objective of identifying functional defects, unexpected behaviors, and risk-prone areas prior to structured validation. This phase emphasized rapid learning of system behavior, informed defect discovery, and early risk assessment to guide subsequent testing activities.

Exploratory testing was intentionally positioned as a complement to manual scripted testing, allowing the team to probe system behavior beyond predefined requirements and to observe how the system responded under varied and imperfect user interactions.

## Test Approach

Exploratory testing was conducted using a pair-testing model to maximize coverage, defect detection, and shared system understanding within a limited execution window. The team was divided into two pairs, each assigned responsibility for distinct functional areas, enabling parallel exploration and minimizing redundant effort.

Before execution, testers reviewed the high-level functional requirements outlined in Appendix B to establish baseline expectations for correct system behavior. Given the constrained scope and simulated nature of the ATM system, additional guidance was taken from the System Under Test and Familiarization with the ATM System sections of the assignment documentation to ensure alignment with intended operational flows.

Rather than attempting exhaustive validation, this phase deliberately prioritized breadth over depth, focusing on core functionalities and representative corner cases. This trade-off enabled rapid exposure of defects and behavioral inconsistencies while deferring exhaustive requirement verification to the manual scripted testing phase. As exploratory testing is inherently unscripted and adaptive, coverage decisions were continuously refined based on observed system responses and emerging risk areas.

## Pair Assignment and Functional Focus

Two exploratory testing pairs were formed with clearly defined functional ownership:

- **Pair 1 (Maheen & Dipu)** focused on system startup and shutdown behavior, session handling (including card insertion and PIN validation), and deposit-related functionality.

- **Pair 2 (Jasdeep & Dhruvi)** focused on withdrawal operations, inter-account fund transfers, and balance inquiry functionality.

This functional partitioning allowed independent subsystems to be explored concurrently while maintaining accountability for coverage and defect reporting within each functional area.

## Test Strategy

Exploratory testing was guided by a set of complementary strategies intended to expose both nominal and adverse system behavior:

- **Common-Path Testing**  
  Validation of typical user workflows involving valid credentials and successful transactions, establishing a baseline for expected system operation.

- **Boundary Testing**  
  Examination of system behavior at operational limits, including withdrawal constraints, minimum deposit values, and account balance thresholds, where defects are commonly observed.

- **Exception-Path Testing**  
  Deliberate triggering of error conditions such as invalid PIN entries, insufficient account balances, unreadable cards, and transaction cancellations to assess robustness and error handling.

- **State-Transition Testing**  
  Verification of correct behavior across system state changes, including power on/off transitions, session initiation and termination, and transaction flow progression.

These strategies ensured that exploratory testing addressed both functional correctness and system resilience.

## Test Case Generation

Test scenarios were dynamically derived throughout execution based on:

- Functional requirements specified in Appendix B
- Use cases outlined in Appendix C
- Expected behavior informed by standard ATM usage patterns
- Anticipated failure modes and edge-case conditions observed during testing

Scenario generation remained intentionally flexible, allowing testers to adapt in real time as new behaviors, defects, or inconsistencies were identified.

## Documentation Approach

Each testing pair maintained real-time records of executed scenarios, observed system outputs, and identified defects. All defects were documented with precise reproduction steps, clearly articulated expected and actual outcomes, severity classification, and references to the affected system functionality. Coverage notes were also maintained to track explored areas and reduce overlap between pairs.

This documentation-first approach ensured that exploratory findings could be reliably reproduced, reviewed, and retested during later testing phases.

## Outcome of the Exploratory Testing Phase

This exploratory testing plan enabled early identification of defects that may not be explicitly addressed by predefined test cases, particularly those related to edge conditions, error handling, and state transitions. The insights gained during this phase informed subsequent manual scripted testing and regression testing, contributing to more focused and effective validation of the ATM simulation system.

---

# 3 Comparative Evaluation of Exploratory and Manual Functional Testing

Exploratory testing and manual functional (scripted) testing were employed as complementary validation strategies to assess the correctness, robustness, and behavioral consistency of the ATM simulation system. Although both approaches aim to identify defects, they operate under fundamentally different testing paradigms and therefore expose distinct classes of faults. This section presents a comparative evaluation grounded in empirical observations from testing activities conducted on versions 1.0 and 1.1 of the system.

## Methodological Comparison

| Dimension | Exploratory Testing | Manual Functional (Scripted) Testing |
|--------|--------------------|-------------------------------------|
| Primary Intent | Early-stage defect discovery and behavioral risk identification | Formal verification of conformance to documented requirements |
| Control Structure | Tester-driven, adaptive exploration guided by observed system behavior | Deterministic execution governed by predefined test cases |
| Planning Overhead | Minimal upfront planning; emphasis on real-time reasoning | Significant upfront specification of inputs, steps, and expected outputs |
| Coverage Dynamics | Broad, non-uniform coverage across features and system states | Structured, requirement-aligned coverage of specified scenarios |
| Defect Characteristics | State-transition faults, boundary-condition failures, error-handling inconsistencies | Functional deviations, incorrect outputs, and missing requirement implementations |
| Repeatability | Lower unless supported by disciplined documentation | High, due to standardized execution and clearly defined oracles |
| Tester Dependence | High; effectiveness strongly correlated with tester expertise | Moderate; process-driven execution reduces individual variability |
| Role in Regression | Limited applicability | Central to regression verification |

## Empirical Findings from the ATM System

Within the context of this project, exploratory testing proved particularly effective at exposing defects related to exception handling, cancellation paths, and transitions between operational states. Its adaptive nature enabled testers to pursue anomalous system responses and investigate behaviors not explicitly anticipated in the predefined test suite. As a result, exploratory testing contributed significantly to early defect discovery and risk identification.

Manual functional testing, in contrast, provided a rigorous and repeatable mechanism for validating the system against its specified functional requirements. Execution of the predefined test suite ensured consistent verification of all core ATM operations, including transaction processing, session management, and error handling scenarios explicitly described in the requirements. Furthermore, manual scripted testing formed the foundation of regression testing for version 1.1, enabling systematic verification of previously reported defects and identification of regressions introduced during defect remediation.

From an efficiency standpoint, exploratory testing delivered high early value with relatively low setup cost, while manual functional testing required greater preparation effort but yielded stronger assurance of requirement compliance and behavioral stability.

## Synthesis and Strategic Implications

The results of this evaluation indicate that exploratory and manual functional testing address complementary quality attributes. Exploratory testing excels at uncovering unanticipated behaviors and defect patterns that emerge from complex interactions and state transitions, making it particularly valuable during early testing phases. Manual functional testing, by contrast, is indispensable for requirement validation, defect verification, and regression testing, where repeatability and traceability are critical.

When applied together, these approaches provided broader defect coverage and deeper insight into system behavior than either method could achieve in isolation. Their combined use enabled both rapid defect discovery and systematic validation, strengthening confidence in the correctness and reliability of the ATM simulation system.

---

# 4 Notes and Discussion of the Peer Reviews of Defect Reports

After completion of the exploratory and manual scripted testing phases, a structured peer review of all defect reports was conducted to improve report quality, consistency, and reproducibility. The peer review process was intentionally performed before regression testing to ensure that all defects entering the verification phase met a uniform standard of documentation and severity classification.

Each testing pair independently reviewed the defect reports authored by the other pair using the defect tracking system. The review focused on evaluating report quality rather than rediscovering defects. Specifically, reviewers assessed the completeness and clarity of reproduction steps, the correctness and precision of expected versus actual outcomes, the appropriateness of severity and priority assignments, accurate version tagging (v1.0 vs. v1.1), and adherence to the reporting guidelines defined in the assignment.

The peer review process identified several recurring issues typical of early-stage defect reporting. Some reports required refinement of reproduction steps to remove ambiguity and ensure consistent reproducibility. In a small number of cases, severity classifications were revised to better reflect the functional impact of the defect rather than its frequency of occurrence. Additionally, overlapping defect reports were identified where similar issues had been independently discovered by both pairs during exploratory testing.

Based on peer feedback, corrective actions were applied to the defect repository. Duplicate and overlapping defects were consolidated into single canonical reports, reproduction steps were expanded and clarified where necessary, and severity levels were adjusted to improve consistency across the defect set. These revisions resulted in clearer, more actionable defect reports that could be reliably retested during regression testing.

Overall, the peer review process significantly improved the quality and maintainability of the defect repository. It ensured consistency in defect classification, enhanced reproducibility, and established a shared understanding of defect impact across the team. This review phase contributed directly to more efficient regression testing and reinforced the importance of disciplined defect reporting as a critical component of the software testing lifecycle.

---

# 5 How the Pair Testing Was Managed and Team Work/Effort Was Divided

Pair testing was adopted as the primary collaboration model throughout all testing phases to enhance defect detection, reduce individual oversight, and promote shared understanding of system behavior. Responsibilities were deliberately structured to balance execution efficiency with analytical rigor and reporting quality.

## Pair Testing Structure and Role Allocation

During the exploratory testing phase, the team was divided into two pairs, each operating at a single workstation. Within each pair, roles were explicitly defined and enforced:

- **Driver (Test Executor):** Responsible for interacting directly with the system under test, executing test scenarios, and navigating transaction workflows.
- **Observer (Reviewer/Recorder):** Responsible for monitoring system behavior, identifying deviations from expected outcomes, validating results, and documenting defects in the defect tracking system.

Roles were periodically rotated within each pair to ensure balanced participation, mitigate role fatigue, and allow all team members to gain experience in both execution and analytical review. This rotation also helped surface defects that may have been overlooked under a single-role perspective.

## Coordination and Communication

Coordination between pairs was maintained through brief synchronization discussions conducted before and after testing sessions. These discussions were used to clarify functional coverage boundaries, share observations of anomalous behavior, and minimize redundant exploration of the same system components. When ambiguities arose regarding expected system behavior, the team collectively referred to the assignment specification and system requirements to establish a consistent and defensible interpretation.

This structured communication ensured alignment across pairs while preserving independent exploration during testing execution.

## Manual Scripted and Regression Testing Collaboration

During the manual scripted testing phase, the team transitioned from pair-based execution to a fully collaborative testing model. Responsibilities for test execution, defect documentation, and test progression coordination were dynamically assigned and rotated among team members. This approach ensured consistent execution of the predefined test suite, adherence to reporting standards, and shared accountability for coverage.

For regression testing, previously reported defects were distributed among team members for individual verification against version 1.1 of the system. Each member was responsible for retesting assigned defects, updating defect status, and documenting outcomes in the tracking system. Any newly discovered defects were reviewed collectively before being reported to ensure they had not been previously identified and to maintain consistency in classification.

## Assessment of the Pair Testing Approach

The pair testing methodology proved effective in improving both test coverage and defect report quality. Real-time review during execution reduced the likelihood of missed defects, while collaborative decision-making improved consistency in defect severity classification and documentation. The structured division of effort, role rotation, and continuous communication contributed to efficient testing execution and increased confidence in the completeness and reliability of the testing results.

---

# 6 Difficulties Encountered, Challenges Overcome, and Lessons Learned

The testing effort revealed a set of technical and process-related challenges that required deliberate resolution to maintain consistency, accuracy, and efficiency across testing phases. Rather than treating these issues as isolated obstacles, they were addressed through structured mitigation strategies that improved the rigor of the testing workflow and the quality of the resulting defect artifacts. The challenges encountered and the corresponding lessons learned are summarized below.

## Key Challenges and Mitigation

- **Ambiguity in expected system behavior** for exceptional scenarios (e.g., transaction cancellations, invalid input handling, and state transitions) due to implicit or underspecified requirements.  
  **Mitigation:** A shared interpretation framework was established through repeated reference to the assignment specification and use case documentation, ensuring consistent decision-making across testers.

- **Inconsistent defect severity and priority classification** during early testing stages, particularly for defects affecting usability versus core transactional correctness.  
  **Mitigation:** Structured peer review and calibration discussions were used to align severity definitions and improve consistency across the defect repository.

- **Coverage and time management challenges** inherent to exploratory testing, where unrestricted exploration risked disproportionate focus on isolated behaviors.  
  **Mitigation:** Functional partitioning between testing pairs and continuous awareness of coverage boundaries helped maintain balanced exploration.

- **Risk of fragmented system understanding** when defects were discovered independently by different testers.  
  **Mitigation:** Pair testing, real-time discussion, and post-session synchronization ensured shared understanding and reduced individual bias.

## Lessons Learned

- Exploratory and scripted testing provide complementary value, combining rapid discovery of unanticipated behaviors with systematic requirement verification.
- Disciplined defect documentation is foundational, directly enabling effective regression testing and reliable defect verification.
- Severity classification requires alignment, not intuition alone; shared criteria improve defensibility and prioritization.
- Structured collaboration mechanisms including pair testing, peer review, and role rotation significantly enhance defect detection accuracy and reporting quality.
- Process discipline scales quality, particularly in multi-phase testing workflows involving regression validation.

---

# 7 Comments and Feedback on the Lab and Lab Document Itself

Overall, the lab provided a well-structured and effective framework for applying software testing and defect tracking concepts in a practical setting. The combination of exploratory testing, manual scripted testing, and regression testing closely reflected real-world testing workflows and reinforced the importance of using multiple testing strategies to achieve comprehensive coverage.

The lab documentation was generally clear and logically organized, particularly the detailed system description, use cases, and predefined test suite. These components provided a solid foundation for planning and executing testing activities. The requirement to use an industry-standard defect tracking tool further enhanced the realism and educational value of the assignment.

One area for potential improvement is the clarification of expected behavior for certain exceptional scenarios, such as transaction cancellation flows and edge-case handling. Providing additional guidance or examples for these cases could reduce ambiguity and help ensure more consistent interpretation across testing teams. Minor inconsistencies in test case numbering were also observed, and addressing these would improve usability of the test suite.

Despite these minor issues, the lab was a valuable and meaningful exercise that strengthened understanding of software testing methodologies, defect reporting practices, and collaborative testing workflows. The assignment effectively balanced technical rigor with practical applicability and served as a strong introduction to systematic software quality assurance.

