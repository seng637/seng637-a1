> **SENG 637 - Software Testing, Reliability, and Quality**

**Lab. Report \#1 â€“ Introduction to Testing and Defect Tracking**

| Group: 9 |
| -------- |
| Maheen   |
| Dipu     |
| Jasdeep  |
| Dhruvi   |

**Table of Contents**

[1 Introduction](#introduction)

[2 High-level description of the exploratory testing plan](#high-level-description-of-the-exploratory-testing-plan)

[3 Comparison of exploratory and manual functional testing](#_Toc439194679)

[4 Notes and discussion of the peer reviews of defect reports](#_Toc439194680)

[5 How the pair testing was managed and team work/effort was divided](#_Toc439194681)

[6 Difficulties encountered, challenges overcome, and lessons
learned](#_Toc439194682)

[7 Comments/feedback on the lab and lab document itself](#_Toc439194683)

# Introduction

This assignment provided hands-on experience with software testing methodologies by testing an ATM simulation system, written in Java. The lab consisted of three main testing phases: exploratory testing, manual scripted testing, and regression testing.

Prior to this lab, our understanding of testing was primarily theoretical. It was known to us that exploratory testing involves ad-hoc testing without predefined test cases, allowing testers to freely explore the system based on their understanding and intuition. Manual functional testing, on the other hand, involves executing predefined test cases systematically to verify that the system meets its requirements. This lab allowed us to apply these concepts practically and understand their differences in real-world scenarios.

The system under test was an ATM simulation system with two versions (1.0 and 1.1), and we used Jira as our defect tracking system to document and manage all discovered defects throughout the testing process.

Throughout this assignment, we referred to the assignment description document, as it contained not only the requirements of the system under test (SUT), but also a guideline for which features to test.

# High Level Description of the Exploratory Testing Plan

Our exploratory testing approach was designed to maximize coverage by leveraging the pair testing. The plan consisted of the following key elements:

## Test Approach

We divided our team into two pairs, with each pair focusing on different functional areas to ensure comprehensive coverage within the 30-minute timeframe. In order to have an understanding of the system for exploratory system, we first reviewed the high-level requirements of the system described in the appendix B of the assignment description. However, since this is only a mock ATM system with limited capabilities, we referred to the "System Under Test" section and "Familiarization with the ATM System" subsection of the assignment document to plan the exploratory testing. Rather than testing all functions in depth (which would be done in the scripted testing phase), we adopted a simpler approach that tests out the basic functionalities, with some corner-cases that we deemed to be necessary at the moment. Again, since this is more or less an ad-hoc testing, not all operation scenarios were considered in this case. 

## Pairs and Functionality Focus

For exploratory testing we created two pairs - Maheen & Dipu in pair 1, and Jasdeep & Dhruvi in pair 2.

- **Pair 1 Focus**: System startup and shutdown procedures, session management (card insertion and PIN validation), and deposit operations.
- **Pair 2 Focus**: Withdrawal operations, transfer operations, and balance inquiry.

## Test Strategy

Our strategy emphasized:

1. **Common Path Testing**: First testing the most typical user workflows (successful transactions with valid inputs)
2. **Boundary Testing**: Testing limits such as maximum withdrawal amounts, minimum deposits, and account balance boundaries
3. **Exception Path Testing**: Deliberately triggering error conditions such as invalid PINs, insufficient funds, and transaction cancellations
4. **State Transition Testing**: Verifying proper system behavior when transitioning between different states (on/off and transaction types)

## Test Case Generation

Test cases were generated based on:

- Requirements outlined in Appendix B of the assignment
- Use cases provided in Appendix C
- Our intuition and experience with ATM systems (ad-hoc)
- Potential error scenarios and edge cases

## Documentation Approach

Each pair maintained real-time documentation of:

- Test scenarios executed
- Observed system behavior
- Defects discovered with complete reproduction steps
- Areas of the system covered

This exploratory plan allowed us to uncover defects that might not be captured by scripted test cases while maintaining systematic coverage of the system's key functionalities.

# Test Plan Details

## Test Types

### Exploratory Testing (Manual Non-Scripted)

- **Purpose**: To explore the functionalities of the system and uncover defects through ad-hoc testing without predefined test cases
- **Duration**: Approximately 30 minutes per pair
- **Approach**: Testers explored the system freely based on their understanding of requirements
- **Documentation**: Defects discovered were logged immediately in the bug tracking system

### Manual Scripted Testing

- **Purpose**: Systematically verify system functionality against predefined test cases
- **Test Suite**: 17 test cases provided in Appendix C of the assignment
- **Coverage**:
  - System Startup and Shutdown (Test Cases 1-4)
  - Session Management (Test Cases 5-11)
  - Withdrawal Operations (Test Cases 12-17)
- **Documentation**: Defects were prefixed with "MFT:" in the summary field to distinguish them from exploratory testing defects

### Regression Testing

- **Purpose**: Verify bug fixes and identify new defects introduced in version 1.1
- **Scope**: Retest all defects found in version 1.0 and execute the scripted test suite again on version 1.1
- **Documentation**: Updated defect status (Resolved/Fixed or In-Progress) and reported new defects specific to version 1.1

## Scope of Testing

### In-Scope Functionalities

**System Operations**

- System startup with initial cash amount entry
- System shutdown when not servicing customers
- Connection establishment with the bank

**Session Management**

- ATM card reading and validation
- PIN entry and validation (including invalid PIN handling)
- Multi-transaction sessions
- Session termination and card ejection

**Transaction Types**

- **Withdrawal**: Account selection, amount selection, cash dispensing, receipt printing
- **Deposit**: Account selection, amount entry, envelope insertion
- **Transfer**: Source and destination account selection, amount entry
- **Balance Inquiry**: Account selection, balance display

**Security Features**

- Invalid PIN handling (3 attempts maximum)
- Card retention after failed authentication
- Transaction logging (excluding PINs)

**Error Handling**

- Insufficient cash in ATM
- Insufficient balance in account
- Unreadable card rejection
- Transaction cancellation

### Out-of-Scope

- Bank-side validation logic (external system)
- Physical hardware components
- Network communication protocols
- Database integrity
- Performance and load testing
- Security penetration testing

### Test Environment

- **System Under Test**: ATM System JAR files (v1.0 and v1.1)
- **Platform**: Windows 10, JDK 17.0.15.6
- **Test Data**:
  - Card Number: 1
  - PIN: 42
  - Initial Balances: Checking $100, Savings $1,000, Money Market $5,000
- **Defect Tracking**: Atlassian Jira or Azure DevOps

## Test Logistics

### Team Structure and Role Assignment

#### Exploratory Testing Phase

**Pair 1**:

- **Tester**: Operates the system and executes test scenarios
- **Recorder**: Documents defects, observations, and test coverage
- **Focus Areas**: System startup/shutdown, session management, withdrawal operations

**Pair 2**:

- **Tester**: Operates the system and executes test scenarios
- **Recorder**: Documents defects, observations, and test coverage
- **Focus Areas**: Deposit operations, transfer operations, and balance inquiry

#### Manual Scripted Testing Phase

In the manual testing phase, all team members worked in unison. The roles for test executor, recorder, and organizer were rotated periodically. At a given moment, two members were in execution, one in recording (writing the found defect in Jira), and one in coordination (making sure that execution is being performed properly, and bug reports are being written with adherence to the given assignment instructions).

We executed all 40 test cases sequentially, as described in the assignment description:

- Test Cases 1-4: System startup and shutdown
- Test Cases 5-11: Session management
- Test Cases 12-18: Withdrawal operations
- Test Cases 19-25: Deposit operations
- Test Cases 26-32: Transfer operations
- Test Cases 33-35: Balance inquiry operations
- Test Cases 36-40: Invalid pin handling

(Note that we found some inconsistencies in the numbering order, but the aforementioned list should give a general idea of the test cases)

#### Regression Testing Phase

- **Defect Verification**: Divided among all group members
- Each member retested assigned defects from version 1.0 in version 1.1
- **New Defect Discovery**: Combined group activity re-executing the scripted test suite
- **Status Updates**: Each member updated status of their assigned defects

### Defect Reporting Guidelines

All defect reports included:

1. Function being tested
2. Initial state of the system
3. Detailed steps to reproduce
4. Expected outcome
5. Actual outcome
6. Priority/Severity (Low, Medium, High, Critical)
7. Version of SUT (1.0 or 1.1)
8. Phase identifier (exploratory or "MFT:" prefix for manual scripted testing)

# Comparison of exploratory and manual functional testing

_[To be written based on your actual testing experience. Consider comparing aspects such as:]_

- _Effectiveness: Which approach found more defects or different types of defects?_
- _Efficiency: Time investment vs. defects discovered_
- _Coverage: Which areas of the system were better tested by each approach?_
- _Advantages and disadvantages of each method_
- _When each approach is most appropriate_
- _Your team's observations and insights from the lab_

**Note**: You need to submit a report generated by your defect tracking system, containing all defects recorded in the system.

# Notes and discussion of the peer reviews of defect reports

_[To be written based on your peer review process. Include:]_

- _How the peer review process was conducted_
- _Key findings from reviewing each pair's defect reports_
- _Any inconsistencies or issues identified during review_
- _How defects were consolidated or deduplicated_
- _Improvements made to defect reports based on peer feedback_

# How the pair testing was managed and team work/effort was divided

_[To be written based on your actual team collaboration. Include:]_

- _Specific responsibilities of each team member_
- _How pairs coordinated during exploratory testing_
- _Communication methods used_
- _How the team handled disagreements or ambiguous situations_
- _Division of work during regression testing_
- _Overall assessment of the pair testing methodology_

# Difficulties encountered, challenges overcome, and lessons learned

_[To be written based on your actual experience. Consider discussing:]_

- _Technical difficulties with the SUT or tools_
- _Challenges in understanding requirements or expected behavior_
- _Issues with the defect tracking system_
- _Time management challenges_
- _How your team overcame these difficulties_
- _Key lessons learned about testing methodologies_
- _Insights gained about defect tracking and reporting_

# Comments/feedback on the lab and lab document itself

_[To be written based on your experience. Provide constructive feedback on:]_

- _Clarity of the lab instructions_
- _Usefulness of the assignment for learning testing concepts_
- _Suggestions for improvement_
- _Aspects that were particularly valuable or challenging_
- _Overall assessment of the lab experience_
